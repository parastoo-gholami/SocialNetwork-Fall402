{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/dataset.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "titles_list = df['title'].dropna().tolist()\n",
    "descriptions_list = df['description'].dropna().tolist()\n",
    "tags_list = df['tags'].dropna().tolist()\n",
    "\n",
    "# Combine titles, descriptions, and tags into a single string\n",
    "combined_content = ' '.join(titles_list + descriptions_list + tags_list)\n",
    "\n",
    "# Tokenize the content\n",
    "content_tokens = word_tokenize(combined_content)\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english') )\n",
    "filtered_content_tokens = [word for word in content_tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "# Join the remaining tokens into phrases for sentiment analysis\n",
    "phrases = ' '.join(filtered_content_tokens)\n",
    "\n",
    "# Sentiment Analysis with VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_score = sia.polarity_scores(phrases)\n",
    "\n",
    "# Print results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on tag and most frequent keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "tags_list = df['tags'].dropna().tolist()\n",
    "\n",
    "tags_content = ' '.join(tags_list)\n",
    "\n",
    "# Tokenize the content\n",
    "tags_tokens = word_tokenize(tags_content)\n",
    "\n",
    "additional_stop_words = ['https', 'http', 'YouTube', 'video', 'get', 'channel', 'na', 'new', 'Instagram',\n",
    "                         'us', 'like', 'Official', '2024', 'Twitter', 'Facebook', 'de', 'vs', 'Video', 'None',\n",
    "                         'first', 'one', 'Watch','youtube', 'official', 'instagram', 'subscribe', 'watch',\n",
    "                         'twitter', 'world', 'facebook', 'news', 'best', 'follow', 'videos', 'none', 'show',\n",
    "                         'live', 'free', 'love', 'time','check', 'content', 'merch', 'tiktok', 'make', 'full',\n",
    "                         'use', 'go', 'things', 'take', 'every', 'got', 'find', 'know', 'highlights', 'day',\n",
    "                         '2','latest', 'today', 'company', 'last', 'back', 'want', '10', 'visit', 'code', '5',\n",
    "                         'yes', 'shop', 'social', 'life', 'night', 'see', 'people', 'website', 'favorita',\n",
    "                         'play',  'en', 'link', 'way', 'thanks', 'stream', 'week', 'podcast', 'much', 'el',\n",
    "                         'good', 'drag', 'also','could', 'app', 'la', 'try', 'enough', 'home', 'never', 'still',\n",
    "                         'director', '3', 'episode', 'let', 'sent',  'impact', 'tumbler', 'white', 'green', 'game'\n",
    "                         , '21', 'du', 'football', 'generals', 'press','grande', 'fans']\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english') + additional_stop_words)\n",
    "filtered_tags_tokens = [word for word in tags_tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "# Count the frequency of each token\n",
    "tags_word_freq = Counter(filtered_tags_tokens)\n",
    "\n",
    "# Select the top 25 keywords\n",
    "top_keywords = [word for word, freq in tags_word_freq.most_common(25)]\n",
    "\n",
    "# Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "positive_keywords = 0\n",
    "negative_keywords = 0\n",
    "neutral_keywords = 0\n",
    "for keyword in top_keywords:\n",
    "    sentiment_score = sia.polarity_scores(keyword)['compound']\n",
    "    if sentiment_score >= 0.05:\n",
    "        positive_keywords += 1\n",
    "    elif sentiment_score <= -0.05:\n",
    "        negative_keywords += 1\n",
    "    else:\n",
    "        neutral_keywords += 1\n",
    "\n",
    "\n",
    "print(f\"Top 25 Keywords (excluding stop words): {top_keywords}\")\n",
    "print(f\"Positive Keywords: {positive_keywords}\")\n",
    "print(f\"Negative Keywords: {negative_keywords}\")\n",
    "print(f\"Neutral Keywords: {neutral_keywords}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
